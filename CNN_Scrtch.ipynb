{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJchm6jYTFtY"
      },
      "outputs": [],
      "source": [
        "# more optimized\n",
        "import numpy as np\n",
        "\n",
        "class Conv2D:\n",
        "  \"\"\"\n",
        "  Simple implementation of a 2D convolutional layer.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_channels, output_channels, kernel_size, padding='valid', activation='relu'):\n",
        "    \"\"\"\n",
        "    Initialize the convolutional layer.\n",
        "\n",
        "    Args:\n",
        "      input_channels: Number of channels in the input data.\n",
        "      output_channels: Number of filters (feature maps) to learn.\n",
        "      kernel_size: Size of the convolutional kernel (filter).\n",
        "      padding: Padding mode ('valid' or 'same').\n",
        "      activation: Activation function to apply (e.g., 'relu', 'sigmoid').\n",
        "    \"\"\"\n",
        "\n",
        "    self.input_channels = input_channels\n",
        "    self.output_channels = output_channels\n",
        "    self.kernel_size = kernel_size\n",
        "    self.padding = padding\n",
        "    self.activation = activation\n",
        "\n",
        "    # Initialize weights and biases with random values\n",
        "    self.weights = np.random.randn( output_channels,input_channels, kernel_size, kernel_size) / np.sqrt(input_channels * kernel_size * kernel_size)\n",
        "    #self.weights = self.weights.reshape((kernel_size,kernel_size,input_channels))\n",
        "    self.bias = np.zeros((output_channels,))\n",
        "    #print(f\"====weight \\n {self.weights}  \\n\\n ====Type \\n {self.weights.dtype}\\n\\n =====shape\\n {self.weights.shape}\")\n",
        "\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    Perform the forward pass of the convolution operation.\n",
        "\n",
        "    Args:\n",
        "      X: Input data (tensor of shape (batch_size, input_height, input_width, input_channels)).\n",
        "\n",
        "    Returns:\n",
        "      Output of the convolutional layer (tensor of appropriate shape based on padding).\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size, input_height, input_width, _ = X.shape\n",
        "\n",
        "    # Handle padding\n",
        "    if self.padding == 'same':\n",
        "      pad_amount = (self.kernel_size - 1) // 2\n",
        "      X_padded = np.pad(X, ((0,0),(pad_amount, pad_amount), (pad_amount, pad_amount), (0, 0)), mode='constant')\n",
        "    else:\n",
        "      X_padded = X\n",
        "\n",
        "    output_height = input_height - self.kernel_size + 1 if self.padding == 'valid' else input_height\n",
        "    output_width = input_width - self.kernel_size + 1 if self.padding == 'valid' else input_width\n",
        "\n",
        "    # Perform convolution for each output channel\n",
        "    output = np.zeros((batch_size, output_height, output_width, self.output_channels))\n",
        "    for n in range(batch_size):\n",
        "      for out_channel in range(self.output_channels):\n",
        "        for i in range(output_height):\n",
        "          for j in range(output_width):\n",
        "            region=X_padded[n,i:i+self.kernel_size,j:j+self.kernel_size,:]\n",
        "            output[n, i, j, out_channel] = np.sum(region * self.weights[out_channel,:,:,:].transpose(1,2,0)) +self.bias[out_channel]\n",
        "\n",
        "    # Apply activation function\n",
        "    if self.activation == 'relu':\n",
        "      output = np.maximum(output, 0)\n",
        "    elif self.activation == 'sigmoid':\n",
        "      output = 1 / (1 + np.exp(-output))\n",
        "    else:\n",
        "      raise NotImplementedError(f\"Activation func '{self.activation}' not supported\")\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "class MaxPooling:\n",
        "  \"\"\"\n",
        "  Simple implementation of a max pooling layer.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, pool_size):\n",
        "    \"\"\"\n",
        "    Initialize the max pooling layer.\n",
        "\n",
        "    Args:\n",
        "      pool_size: Size of the pooling window.\n",
        "    \"\"\"\n",
        "\n",
        "    self.pool_size = pool_size\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    Perform the forward pass of the max pooling operation.\n",
        "\n",
        "    Args:\n",
        "      X: Input data (tensor of shape (batch_size, input_height, input_width, input_channels)).\n",
        "\n",
        "    Returns:\n",
        "      Output of the max pooling layer (tensor of appropriate shape).\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size, input_height, input_width, input_channels = X.shape\n",
        "    output_height = input_height // self.pool_size\n",
        "    output_width = input_width // self.pool_size\n",
        "\n",
        "    output = np.zeros((batch_size, output_height, output_width, input_channels))\n",
        "    for n in range(batch_size):\n",
        "      for i in range(output_height):\n",
        "        for j in range(output_width):\n",
        "          # Extract region\n",
        "          region = X[n, i * self.pool_size:(i +1) * self.pool_size, j * self.pool_size:(j+1) * self.pool_size,:]\n",
        "          # Find maximum value and store\n",
        "          output[n,i,j,:]=np.max(region,axis=(0,1))\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "class SimpleCNN:\n",
        "  def __init__(self):\n",
        "    # Define layer parameters (replace with desired values)\n",
        "    self.conv1 = Conv2D(3, 16, 3, padding='same', activation='relu')\n",
        "    self.pool1 = MaxPooling(2)\n",
        "    self.conv2 = Conv2D(16, 32, 3, padding='tanh', activation='relu')\n",
        "    self.pool2 = MaxPooling(2)\n",
        "    self.conv3 = Conv2D(32, 64, 3, padding='valid', activation='relu')\n",
        "\n",
        "  def forward(self, X):\n",
        "    # Pass data through each layer sequentially\n",
        "    X = self.conv1.forward(X)\n",
        "    X = self.pool1.forward(X)\n",
        "    X = self.conv2.forward(X)\n",
        "    X = self.pool2.forward(X)\n",
        "    X = self.conv3.forward(X)\n",
        "    return X\n",
        "\n",
        "# Example usage\n",
        "model = SimpleCNN()\n",
        "batch_size = 5\n",
        "input_data = np.random.randn(batch_size, 20, 20, 3)  # Replace with your actual data\n",
        "output = model.forward(input_data)\n",
        "\n",
        "print(output.shape)  # Output shape will depend on your chosen pool sizes\n",
        "output.item,output.size,output.ndim"
      ]
    }
  ]
}